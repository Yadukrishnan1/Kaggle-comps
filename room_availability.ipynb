{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "room_availability.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAWigIfOWyOSRwQqzj13aI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yadukrishnan1/Kaggle-comps/blob/main/room_availability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "StKYMzoT1PSq"
      },
      "outputs": [],
      "source": [
        "# Python 3 environment \n",
        "import numpy as np              # Linear algebra\n",
        "import pandas as pd             # Data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns           # Data visualization\n",
        "import matplotlib.pyplot as plt # Data visualization\n",
        "import os                       # Operating System library\n",
        "from os import path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Libraries\n",
        "import sys\n",
        "!{sys.executable} -m pip install xgboost sklearn\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold, RandomizedSearchCV\n",
        "from xgboost.sklearn import XGBRegressor, XGBClassifier\n",
        "import xgboost as xgb\n"
      ],
      "metadata": {
        "id": "gWWIrtDs8w8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data and exploring to find features and classes"
      ],
      "metadata": {
        "id": "dq_MsLn97r8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('')\n",
        "FEATURES=list(df.columns[0:30]) # Change the columns according to dataset\n",
        "TARGET='yearly_availability'\n",
        "df.head()"
      ],
      "metadata": {
        "id": "blBQMH7N26pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for a quick EDA (Exploratory Data Analysis)"
      ],
      "metadata": {
        "id": "O-uIY21F7yDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EDA(df):\n",
        "  print('The column name, Dtype, and Null-count\\n\\n', df.info())\n",
        "  print('The descriptory statistics of the features and label\\n\\n', df.describe())\n",
        "  print('The features and the label of the data\\n\\n', df.columns)\n",
        "  print('The number of missing values in the data\\n\\n', df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "fDEFESbi3a6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the class imbalance"
      ],
      "metadata": {
        "id": "qfCnJYAy7oFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class imbalance needs to be taken cared of before applying any model\n",
        "\n",
        "print('Hotels-0 fraction in the data :',len(df[df['yearly_availability']==0])/len(df)*100)\n",
        "print('Hotels-1 fraction in the data :',len(df[df['yearly_availability']==1])/len(df)*100)\n",
        "\n",
        "# Class imbalance visualized\n",
        "\n",
        "fig, axes = plt.subplots(1, 1, figsize=(7,5), dpi=100)\n",
        "fig.suptitle(\"Class imbalance\", y=1.1, fontsize=18)\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "g=sns.countplot(data=df, x='Class', ax=axes)\n",
        "axes.set_yscale(\"log\")\n",
        "axes.set_xlabel(\"Class Label\", fontsize=16)\n",
        "axes.set_ylabel(\"Fraction of the labelled data \", fontsize=16)\n",
        "plt.title('Class Distributions \\n (0: No Fraud | 1: Fraud)', fontsize=14)\n",
        "\n",
        "fig.subplots_adjust(left=0., bottom=0., right=1., top=1.0)\n",
        "# plt.savefig('class_imbalance_expedia.png', dpi=300, bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Szz7WjNw6JjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding the categorical variables"
      ],
      "metadata": {
        "id": "Qcxq35qFB47q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = df.select_dtypes(include=['object']).copy()\n",
        "\n",
        "# Encoding the columns\n",
        "\n",
        "enc_make = OrdinalEncoder()\n",
        "\n",
        "cat_df_transformed = enc_make.fit_transform(cat_df)\n",
        "\n",
        "for i,j in enumerate(cat_df.columns):\n",
        "  cat_df[j] = cat_df_transformed.transpose()[i]\n",
        "\n",
        "# Adding converted labels to df\n",
        "for i in df.columns:\n",
        "  if i in cat_df.columns:\n",
        "    df[i] = cat_df[i]\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VFMwbsHlB88Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling of the continuous variables"
      ],
      "metadata": {
        "id": "UEr2j26b9RV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling of features: All columns except amount and time are scaled using PCA.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "rob_scaler = RobustScaler()\n",
        "\n",
        "# We will use Robust scaler because it's ideal if there are outliers\n",
        "\n",
        "df['Amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "df=df[[c for c in df if c not in ['Class']] + ['Class']]"
      ],
      "metadata": {
        "id": "oBOsEYw67mi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier detection "
      ],
      "metadata": {
        "id": "jsyjshoz9dPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier detection for featuresusing the mean and the standard deviation assuming a Gaussian distribution\n",
        "# The features to be used: \n",
        "\n",
        "def outlier_detection(df, feature):\n",
        "  df[feature]=df[feature][(df[feature]>df[feature].mean()-df[feature].std()) & (df[feature]<df[feature].mean()+df[feature].std())]\n",
        "  return df[feature]\n",
        "\n",
        "outlier_detection(df, '')\n"
      ],
      "metadata": {
        "id": "E3WXQVXK9FB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature creation: distance from the hotels to a particular destination, in this case, NYC City center."
      ],
      "metadata": {
        "id": "Kz4CYEFh-Hy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature creation: Distance between NYC and the hotels using the Haversine Formula\n",
        "\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "def haversine(lon1, lat1, lon2, lat2):\n",
        "    \"\"\"\n",
        "    Calculate the great circle distance in kilometers between two points \n",
        "    on the earth (specified in decimal degrees)\n",
        "    \"\"\"\n",
        "    # convert decimal degrees to radians \n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # haversine formula \n",
        "    dlon = lon2 - lon1 \n",
        "    dlat = lat2 - lat1 \n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a)) \n",
        "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
        "    return c * r"
      ],
      "metadata": {
        "id": "60nv2NqQ92ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NYC_lat\t= 40.730610\n",
        "NYC_long=\t-73.935242\n",
        "\n",
        "df['distance_city']=haversine(df[], df[], NYC_long, NYC_lat)"
      ],
      "metadata": {
        "id": "woS0G5H8CVTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stratified K-fold cross-validation of the data due to imbalance**"
      ],
      "metadata": {
        "id": "OFDx0FwB47bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "f1score=[]\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf.split(df[FEATURES], df[TARGET])):\n",
        "    X_train, X_valid = df.iloc[train_idx], df.iloc[valid_idx]\n",
        "    y_train = X_train[TARGET]\n",
        "    y_valid = X_valid[TARGET]\n",
        "    X_train = X_train.drop(TARGET, axis=1)\n",
        "    X_valid = X_valid.drop(TARGET, axis=1)\n",
        "    \n",
        "    cbr = CatBoostClassifier(random_state=42)\n",
        "\n",
        "    cbr =  cbr.fit(X_train, y_train, verbose=False)\n",
        "    y_pred = cbr.predict(X_valid)\n",
        "    f1score.append(f1_score(y_valid, y_pred))\n",
        "    print(f'Fold {fold}: F1: ', f1_score(y_valid, y_pred))"
      ],
      "metadata": {
        "id": "gao9F-9c2-v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-sampling the majority class for balancing the data"
      ],
      "metadata": {
        "id": "2UOdJoCB_dD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of fraud classes are fraud_len\n",
        "\n",
        "avail_df = df[df['yearly_availability'] == 1]\n",
        "avail_len=len(avail_df)\n",
        "nonavail_df = df[df['yearly_availability'] == 0][:avail_len]\n",
        "\n",
        "balanced_df = pd.concat([avail_df, nonavail_df])\n",
        "\n",
        "# Shuffling the data\n",
        "\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=None)\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "CUglOw2U3LIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization using boxplot"
      ],
      "metadata": {
        "id": "5vv9bNaWAPxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier detection using seaborn boxplot\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(6, 5, figsize=(30, 35))\n",
        "\n",
        "count=0\n",
        "for i in range(4):\n",
        "    for j in range(3):\n",
        "        sns.boxplot(ax=axes[i, j],data=balanced_df, x='yearly_availability', y=balanced_df.columns[count])\n",
        "        axes[i,j].set_title(balanced_df.columns[count])\n",
        "        count+=1"
      ],
      "metadata": {
        "id": "YyJSaPL03Nkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection using K-Best"
      ],
      "metadata": {
        "id": "1ANeIoxNBYG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 1, figsize=(15,10))\n",
        "\n",
        "# Sub-sample dataframe\n",
        "sub_sample_corr = balanced_df.corr()\n",
        "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=axes)\n",
        "axes.set_title('Sub-sample Correlation Matrix', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GSonbr8w3QEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=balanced_df.drop(columns = \"yearly_availability\", axis=1)\n",
        "y=balanced_df['yearly_availability']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "0E8w3ayhAwPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi squared feature selection for categorical data\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "\tfs = SelectKBest(score_func=chi2, k='all')\n",
        "\tfs.fit(X_train, y_train)\n",
        "\tX_train_fs = fs.transform(X_train)\n",
        "\tX_test_fs = fs.transform(X_test)\n",
        "\treturn X_train_fs, X_test_fs, fs\n",
        "\n",
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
        "\n",
        "# what are scores for the features\n",
        "for i in range(len(fs.scores_)):\n",
        "\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n",
        " \n",
        "# plot the scores\n",
        "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xoJbn85JBW9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy (in %):\",clf.score(X_test, y_test)*100)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print('F1 score', f1_score(y_test, y_pred))\n",
        "\n",
        "plot_confusion_matrix(clf, X_test, y_test)  \n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aMMUA7UtA8ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance=clf.feature_importances_\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBUf4tBEBABE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=300)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy (in %):\",clf.score(X_test, y_test)*100)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print('F1 score', f1_score(y_test, y_pred))\n",
        "\n",
        "plot_confusion_matrix(clf, X_test, y_test)  \n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UHj8FOqJBDuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance=clf.feature_importances_\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VeDIepB1BFth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=2, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy (in %):\",clf.score(X_test, y_test)*100)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print('F1 score', f1_score(y_test, y_pred))\n",
        "\n",
        "plot_confusion_matrix(clf, X_test, y_test)  \n",
        "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yhpmbWhNBI65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance=clf.feature_importances_\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIEhllA8BL0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df=data.mean()\n",
        "std_df=data.std()\n",
        "\n",
        "for i in data.columns:\n",
        "    df=data[(data > (data.mean() - 2*data.std())) & (data <=(data.mean() + 2*data.std()))]\n",
        "\n",
        "df.dropna()"
      ],
      "metadata": {
        "id": "0lt13FUMDFdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}